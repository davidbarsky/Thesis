\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}
\usepackage{fontspec}
\geometry{letterpaper}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{tgbonum}

\setromanfont{Minion Pro}
\setmonofont[Scale=0.85]{FiraCode-Retina}
\linespread{1.7}

\title{Working Title}
\date{}

\begin{document}
\maketitle

\begin{abstract}

Scheduling a program onto a multi-worker system to minimize cost is a common problem in performance-sensitive contexts. These contexts include any sort of work that can be parallelizable. These problems are often represented in the form of a \emph{Directed Acyclic Graph}. Unfortunately, this problem is also NP-complete. To avoid paying the full price of calculating an NP-complete problem, users have turned to heuristics to approximate ideal solutions. In this thesis, we implement and profile several robust scheduling algorithms, report on their performance, and propose new heuristic and reference implementation.

\end{abstract}

\section{Introduction}

Graphs are remarkably useful data structures, capable of representing networks and relationships between entities. They consist nodes and edges, where \emph{nodes} are values that are connected by \emph{edges}. Directed acyclic graphs (DAGs) are a subtype of graphs. They have two critical distinctions from regular graphs. First, edges connecting nodes have direction, meaning that there is a clear before and after for any given vertex. Second, the acyclic property ensures that there is a consistent, ordered sequence such that it is impossible to start with a node $a$ and end back at node $a$. Therefore, every directed acyclic graph has a valid topological ordering. That is, there exists a linear ordering of its nodes such that for every directed edge $ab$ (an edge connecting node $a$ and $b$), $a$ comes before $b$ in the ordering. Less formally, this means that a directed acyclic graph has a clear, linear order from start to end.

Our research interest in directed acyclic graphs lies in their application in task scheduling.  Task scheduling, generally speaking, is a procedure for allocating work to resources that can complete that work. A scheduler is the component that handles the scheduling activity. Here, a directed acyclic graph would be called a \emph{job}, nodes would be called \emph{tasks}, and edges \emph{communication costs} or \emph{latency}. Historically, most research into task scheduling was focused on multiprocessor systems, where communication costs between tasks is is minimal. With the advent of cloud computing---a model of computing that allows users to rent resources by the hour as opposed to making a large, upfront capital investment---and ``big data''---data set and applications that are too large to reasonably maintain on one computer---users have begun to distribute their workloads across clusters of many machines. Apache Spark, an open-source cluster-computing framework, is perhaps the most famous example of this computing model. 

Our interest lies in the domain of cluster computing. Cluster computing, even in cloud environments, is challenging from both financial and engineering perspectives. While throwing machines at a given job may fix the immediate problem, resource utilization can be costly in terms of both time and cost. At large scales, small inefficiencies tend to have an outsized effect. Therefore, it makes sense for users to find an optimal job schedule ahead of time. Unfortunately, we---the field as a whole---are not aware of techniques that allow for finding an optimal schedule in polynomial time. That is, the difficultly of finding an optimal schedule scales to intractable degrees on sizable inputs. Instead of a running time of $O(n^2)$ where $n$ is the size of graph, finding an optimal schedule is $O(n!)$. The runtime complexity of $O(n!)$ is due to the requirement of examining every permutation of every path in the graph.

In this model, our primary concerns are maintaining parallelism, minimizing cost, and reducing network communication costs. We consider cost to the the complete time spend utilizing computing resources. Network communication, therefore, is idle unused time. Our goal is to minimize unnecessary network communication costs. We have implemented several heuristics and profiled them across a range of graphs, both synthetic and real. The synthetic graphs were from a tool known as GGen, a random graph generator designed for scheduling simulations. The real sample workloads were taken from sample Apache Spark workflows.

\section{Scheduling Algorithms}

\subsection*{Implementation Notes}

We will provide a single running example through this section. Namely, a synthetic graph named an Erdos GMN graph. It appears to be squat featuring multiple critical paths. Critical paths are paths in a directed acyclic graph that go from a source node (a node that has no parents) to a leaf node (a node that has no children). Our system design is a small edit of the directed acyclic graph definition given in the introduction. We consider a whole graph to be an \emph{job} and nodes considered \emph{tasks}. Tasks are scheduled onto \emph{task queues}, which are machines that process tasks. 

\subsection{Edge Zero}

The Edge Zero, or edge-zeroing algorithm selects clusters for merging based on network latency (edges). Its goal is to minimize network latency, so it is most effective on graphs that have high network costs. At each step, the algorithm finds the edge with the largest weight. The two clusters incident by the edge will be merged if the merging (thereby zeroing the largest weight) does not increase the completion time. After two clusters are merged, the ordering of nodes in the resulting cluster is based on the static b-levels of the nodes. The static b-levels of a task is the latest possible start time of a given task.

The edge zero algorithm is an unbounded scheduler, in that there is no bound on the number of virtual machines it will generate as part of the scheduling process. The algorithm works by initially placing each task onto a task queue. The algorithm then iterates through the scheduled, combining machines whenever two tasks can be colocated. At each step, a new schedule is generated hence ``zeroing'' the edges. Each queue is then topologically sorted.

For an Erdos graph of 220 tasks, edge zero generated a schedule of 155 task queues and a final cost of 8638. At 560 tasks, Edge Zero generated a schedule with 479 task queues and a final cost of 13112.

\subsection{Linear Cluster}

The Linear Clustering algorithm merges nodes to form a single cluster based on the critical path. The algorithm first determines the set of tasks constituting the critical, then schedules all the critical path tasks to a single task queue at once. These tasks and all edges incident on them are then removed from the graph. The process repeats until all task are scheduled. Formally:

\begin{enumerate}
\item Initially, mark all tasks as unexamined.
\item Determine the critical path composed of unexamined edges only. We determined the critical paths by find the highest b-level for each layer of the graph.
\item Create a cluster by colocating all the tasks on the critical path.
\item Mark all the edges incident on the critical path and all the edges incident to the
nodes in the cluster as examined.
\end{enumerate}

This process continues until all tasks in the job are examined.

For an Erdos graph of 220 tasks, Linear Custer generated a schedule of 43 task queues and a final cost of 10010. At 560 tasks, Linear Cluster generated a schedule with 76 task queues and a final cost of 16707. It is less efficient than edge zero.

\subsection{Round Robin}

The Round Robin algorithm is one of the simplest algorithms we implemented. It goes through the entire graph, and schedules tasks one-by-one onto a set of task queues, whose size $n$ is determined by the user. It allocate all tasks evenly onto task queues. The graph is iterated over in topological order, such that every task is scheduled onto a task queue modulo its position in the linearized graph.

For an Erdos graph of 220 tasks, round robin generated a schedule of 4 (provided by the user) task queues and a final cost of 14313 (we do no have a unit, it is an arbitrary cost). At 560 tasks, round robin generated a schedule with 4 task queues and a final cost of 15546. This algorithm has a higher initial cost, but is able to scale gracefully as the number of tasks increases.

\subsection{CCA}
\subsection{Birkhoff}
\subsection{Dynamic Critical Path}

\section{Results}

TBA; need further analysis of graphs.

\section{Conclusion}

\section{Related Work}

- Yu-Kwong Kwok and Ishfaq Ahmad

\end{document}  