\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}
\usepackage{fontspec}
\geometry{letterpaper}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{tgbonum}

\setromanfont{Minion Pro}
\setmonofont[Scale=0.85]{FiraCode-Retina}
\linespread{1.7}

\title{Working Title}
\date{}

\begin{document}
\maketitle

\begin{abstract}

Scheduling a program onto a multi-worker system to minimize cost is a common problem in performance-sensitive contexts. These contexts include any sort of work that can be parallelizable. These problems are often represented in the form of a \emph{Directed Acyclic Graph}. Unfortunately, this problem is also NP-complete. Therefore, researchers and engineers have developed heuristics that attempt to find a reasonable approximation of a lowest cost schedule. In this thesis, we implement and profiled several scheduling algorithms that are suited for different graph types, and report our findings here.

\end{abstract}

\section{Introduction}

Graphs are remarkably useful data structures, capable of representing networks and relationships between objects. Directed acyclic graphs (DAGs), a subtype of graphs have two distinct properties that differentiate them from regular graphs. First, edges connecting vertices have direction, meaning that there is a clear ``before'' and ``after'' for any given stage. Secondly, the constraints imposed by ``acyclic'' allow for the representation of connected actions over time. These two properties make DAGs remarkably useful in a whole range of computing tasks, such as the powering the version control system that this thesis is stored in. However, the focus of this thesis is in the domain of task scheduling, which is not a small subfield by any means. Scheduling, particularly static scheduling that occurs prior to execution, is a very common problem faced in both multi-core and cloud environments. Most computation is a mapping from category \emph{A} to category \emph{B}, but that mapping/transformation is composed of individual, partial transformations. With the advent of multicore processors and ``big data''---data sets which are too large to reasonably process on a single machine---throwing more processor cores/machines at a problem is a reasonable solution. However, that strategy is problematic, as utilizing those resources, as resource utilization can be ineffective in both cost and time. Furthermore, at large scales, small inefficiencies tend to have an outsized effect. Therefore, it makes sense for practitioners, operators, and software itself attempt to optimize this step, as a relatively small computation ahead of time can save time and money. However, this optimization step is fraught with challenges. Namely, finding an optimal schedule is NP-hard, which means that for large graphs is problem approaches intractability. To formalize trade-off, one can have a schedule now or an ideal one later. There is no hard-and-fast rule; this is a consideration that must be decided in a local context.

That isn't to say that researchers and engineers haven't attempted to carve out a ``middle way'' between the two---heuristics that approximate an ideal solution are often a compelling solution. These heuristics offer a runtime complexity that is closer to quadratic than exponential. This thesis implements several known heuristics. These heuristics are subject to a battery of experiments, which can be broken into two stages. The first are synthetic workloads, which were generated by GGen, a random DAG generator designed for scheduling simulations. The second were using sample workloads from Apache Spark. We present the results of those experiments and takeaways for engineers and researchers.

The structure of this thesis is as follows. First, the implementation and behavior of the heuristics will be explained. Then, we will present our conclusions and recommendations.

\section{Scheduling Algorithms}

\subsection*{Implementation Notes}

A DAG is a directed graph that has a topological ordering, where a sequence of vertices is ordered from start to finish from within the sequence. DAGs have useful applications in numerous contexts, but this thesis will focus on applications in scheduling. The metaphor used throughout this thesis is that of \emph{Tasks}, \emph{TaskQueues}, and \emph{Virtual Machines}, with network latencies, represented as integer costs between tasks. Tasks are vertices that have dependencies (upstream vertices) and dependents (downstream vertices). Dependents/Dependencies of a task can be accessed through a \texttt{Map<Task, Integer>}, where \texttt{Task} is the destination node and \texttt{Integer} represents the network cost of the edge. We implemented several algorithms which schedule graphs according to different constraints.

There are two main types of schedulers: bounded and unbounded schedulers. Bounded schedulers are bounded by the operator as to how many queues the operator wants. Unbounded schedulers, like the name implies, are not bounded by this constraint. Bounded schedulers have this function signature: \texttt{def generateSchedule(numQueues: Int, machineType: MachineType): List[TaskQueue]}. Unbounded schedulers have the following signature: \texttt{def generateSchedule(numQueues: Int, graph: List[Task], machineType: MachineType): List[TaskQueue]}; differing in the presence of an additional parameter, \texttt{numQueues: Int}.

\subsection{Edge Zero}

The Edge Zero algorithm is an unbounded scheduler, in that there is no bound on the number of virtual machines it will generate as part of the scheduling process. The algorithm works by initially placing each task onto a Virtual Machine. The algorithm then iterates through the the one-to-one mapping of virtual machines, combining machines whenever two tasks can be colocated, hence ``zeroing'' the edges. Each queue is then topologically sorted.

\subsection{Linear Cluster}
First, split the graph into two groups: nodes with neighbors and without. The ones without neighbors are shuffled onto their own task queue. Those can be built without worry. That leaves us with the rest. Of the rest (the dependent nodes) need to scheduled. To do that, we will find the longest path between a source and leaf node. To accomplish this, we will use a BFS (with negative weights determine the ``shortest'' path). On each iteration of the BFS, we'll pass a set of visited nodes. We stop iteration when the set of visited nodes equals the size of the dependent nodes.

\subsection{Round Robin}
We treated Round Robin as a baseline, where a graph of an arbitrary size will be distributed evenly on across a set of queues.

\subsection{CCA}
\subsection{Birkhoff}
\subsection{Dynamic Critical Path}

\section{Results}

TBA; need further analysis of graphs.

\section{Conclusion}

\section{Related Work}

- Yu-Kwong Kwok and Ishfaq Ahmad

\end{document}  